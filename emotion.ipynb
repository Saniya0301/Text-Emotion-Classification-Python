{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "# Step 1: Load and inspect data\n",
        "data = pd.read_csv(\"train.txt\", sep=';', header=None)\n",
        "data.columns = [\"Text\", \"Emotions\"]\n",
        "print(\"Sample data:\")\n",
        "print(data.head())\n",
        "\n",
        "# Step 2: Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Emotion_Label\"] = label_encoder.fit_transform(data[\"Emotions\"])\n",
        "\n",
        "# Step 3: Tokenize and pad text data\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(data[\"Text\"])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data[\"Text\"])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100, padding='post')\n",
        "\n",
        "# Step 4: Split data\n",
        "X = padded_sequences\n",
        "y = data[\"Emotion_Label\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Build model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=64, input_length=100),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "# Step 6: Compile and train\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nTraining model...\")\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 7: Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Optional: Predict on a new example\n",
        "example_text = [\"I feel amazing and full of energy!\"]\n",
        "example_seq = tokenizer.texts_to_sequences(example_text)\n",
        "example_pad = pad_sequences(example_seq, maxlen=100, padding='post')\n",
        "\n",
        "predicted_class = np.argmax(model.predict(example_pad), axis=1)\n",
        "emotion = label_encoder.inverse_transform(predicted_class)\n",
        "\n",
        "print(f\"\\nPrediction: {emotion[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgpwP0tN5mTs",
        "outputId": "8770259f-4a03-476f-9850-00f9f89e4d0f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample data:\n",
            "                                                Text Emotions\n",
            "0                            i didnt feel humiliated  sadness\n",
            "1  i can go from feeling so hopeless to so damned...  sadness\n",
            "2   im grabbing a minute to post i feel greedy wrong    anger\n",
            "3  i am ever feeling nostalgic about the fireplac...     love\n",
            "4                               i am feeling grouchy    anger\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "Epoch 1/5\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.3609 - loss: 1.5650 - val_accuracy: 0.5997 - val_loss: 1.0704\n",
            "Epoch 2/5\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.7635 - loss: 0.7138 - val_accuracy: 0.8334 - val_loss: 0.5172\n",
            "Epoch 3/5\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9507 - loss: 0.1846 - val_accuracy: 0.8394 - val_loss: 0.4827\n",
            "Epoch 4/5\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9858 - loss: 0.0669 - val_accuracy: 0.8459 - val_loss: 0.4686\n",
            "Epoch 5/5\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.9927 - loss: 0.0354 - val_accuracy: 0.8481 - val_loss: 0.5058\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8538 - loss: 0.5197\n",
            "\n",
            "Test Accuracy: 0.85\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\n",
            "Prediction: surprise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"i am ever feeling nostalgic about the fireplace i will know that it is still on the property\"\n",
        "\n",
        "# Define max_length used during training\n",
        "max_length = 100  # Should match the value used in pad_sequences when training\n",
        "\n",
        "# Preprocess the input text\n",
        "input_sequence = tokenizer.texts_to_sequences([input_text])\n",
        "padded_input_sequence = pad_sequences(input_sequence, maxlen=max_length, padding='post')\n",
        "\n",
        "# Predict\n",
        "prediction = model.predict(padded_input_sequence)\n",
        "predicted_label = label_encoder.inverse_transform([np.argmax(prediction[0])])\n",
        "\n",
        "# Output\n",
        "print(f\"Predicted Emotion: {predicted_label[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usKcc0rF65Jg",
        "outputId": "acb07c47-5867-41b0-ab94-ae5eff955a84"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Predicted Emotion: love\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v-Dwj5C57C6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}